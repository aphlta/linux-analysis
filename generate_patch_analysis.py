#!/usr/bin/env python3
import subprocess
import pandas as pd
import re
import argparse
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from openpyxl.styles import Alignment, Font
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl import Workbook
import multiprocessing

# å…¨å±€ç¼“å­˜
patch_id_cache = {}
cache_lock = threading.Lock()

def run_git_command(cmd):
    """æ‰§è¡Œgitå‘½ä»¤å¹¶è¿”å›è¾“å‡º"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip().split('\n') if result.stdout.strip() else []
    except Exception as e:
        print(f"Error running command: {cmd}")
        print(f"Error: {e}")
        return []

def run_git_command_single(cmd):
    """æ‰§è¡Œgitå‘½ä»¤å¹¶è¿”å›å•è¡Œè¾“å‡º"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip() if result.returncode == 0 else None
    except Exception as e:
        print(f"Error running command: {cmd}")
        print(f"Error: {e}")
        return None

def validate_branch_exists(branch_name):
    """éªŒè¯åˆ†æ”¯æ˜¯å¦å­˜åœ¨"""
    cmd = f'git rev-parse --verify {branch_name}'
    result = run_git_command_single(cmd)
    return result is not None

def find_merge_base(branch1, branch2):
    """æ‰¾åˆ°ä¸¤ä¸ªåˆ†æ”¯çš„å…¬å…±ç¥–å…ˆæäº¤"""
    cmd = f'git merge-base {branch1} {branch2}'
    result = run_git_command_single(cmd)
    if result:
        print(f"æ‰¾åˆ°å…¬å…±ç¥–å…ˆæäº¤: {result}")
        return result
    else:
        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ° {branch1} å’Œ {branch2} çš„å…¬å…±ç¥–å…ˆ")
        return None

def clean_text_for_excel(text):
    """æ¸…ç†æ–‡æœ¬ä¸­Excelä¸æ”¯æŒçš„å­—ç¬¦"""
    if not text:
        return ""

    # ç§»é™¤æˆ–æ›¿æ¢Excelä¸æ”¯æŒçš„æ§åˆ¶å­—ç¬¦
    # ä¿ç•™å¸¸è§çš„å¯æ‰“å°å­—ç¬¦å’Œæ¢è¡Œç¬¦
    cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', str(text))

    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…Excelå•å…ƒæ ¼è¿‡å¤§
    if len(cleaned) > 32767:  # Excelå•å…ƒæ ¼æœ€å¤§å­—ç¬¦æ•°
        cleaned = cleaned[:32760] + "...[æˆªæ–­]"

    return cleaned

def get_patch_id(commit_hash):
    """è·å–æäº¤çš„patch-idï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    with cache_lock:
        if commit_hash in patch_id_cache:
            return patch_id_cache[commit_hash]

    cmd = f'git show {commit_hash} | git patch-id --stable'
    result = run_git_command_single(cmd)
    patch_id = None
    if result:
        patch_id = result.split()[0]  # patch-idæ˜¯ç¬¬ä¸€ä¸ªå­—æ®µ

    with cache_lock:
        patch_id_cache[commit_hash] = patch_id

    return patch_id

def build_target_branch_patch_index(target_branch, merge_base=None):
    """æ„å»ºç›®æ ‡åˆ†æ”¯çš„patch-idç´¢å¼•"""
    print(f"æ­£åœ¨æ„å»º {target_branch} åˆ†æ”¯çš„patch-idç´¢å¼•...")

    # å¦‚æœæœ‰å…¬å…±ç¥–å…ˆï¼Œåªè·å–å…¬å…±ç¥–å…ˆä¹‹åçš„æäº¤
    if merge_base:
        target_commits_cmd = f'git log {merge_base}..{target_branch} --format=%H'
        print(f"åªåˆ†æå…¬å…±ç¥–å…ˆ {merge_base[:8]} ä¹‹åçš„æäº¤")
    else:
        target_commits_cmd = f'git log {target_branch} --format=%H'

    target_commits = run_git_command(target_commits_cmd)
    target_commits = [c.strip() for c in target_commits if c.strip()]

    print(f"ç›®æ ‡åˆ†æ”¯æœ‰ {len(target_commits)} ä¸ªæäº¤éœ€è¦å»ºç«‹ç´¢å¼•")

    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè·å–patch-id
    patch_id_index = set()

    def get_patch_id_for_commit(commit):
        return get_patch_id(commit)

    with ThreadPoolExecutor(max_workers=8) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_commit = {executor.submit(get_patch_id_for_commit, commit): commit
                           for commit in target_commits}

        # æ”¶é›†ç»“æœ
        processed = 0
        for future in as_completed(future_to_commit):
            commit = future_to_commit[future]
            try:
                patch_id = future.result()
                if patch_id:
                    patch_id_index.add(patch_id)
                processed += 1

                # æ¯å¤„ç†100ä¸ªæäº¤æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if processed % 100 == 0 or processed == len(target_commits):
                    progress = int(processed / len(target_commits) * 100)
                    print(f"ç´¢å¼•æ„å»ºè¿›åº¦: {progress}% ({processed}/{len(target_commits)})")

            except Exception as e:
                print(f"å¤„ç†æäº¤ {commit} æ—¶å‡ºé”™: {e}")

    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(patch_id_index)} ä¸ªå”¯ä¸€patch-id")
    return patch_id_index

def is_patch_unique_fast(commit_hash, target_patch_index):
    """å¿«é€Ÿæ£€æŸ¥è¡¥ä¸æ˜¯å¦ç‹¬æœ‰ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„ç´¢å¼•ï¼‰"""
    source_patch_id = get_patch_id(commit_hash)
    if not source_patch_id:
        print(f"è­¦å‘Š: æ— æ³•è·å–æäº¤ {commit_hash} çš„patch-id")
        return True  # å¦‚æœæ— æ³•è·å–patch-idï¼Œå‡è®¾æ˜¯ç‹¬æœ‰çš„

    return source_patch_id not in target_patch_index

def get_commit_details(commit_hash):
    """è·å–æäº¤çš„è¯¦ç»†ä¿¡æ¯"""
    # è·å–å®Œæ•´çš„æäº¤æ¶ˆæ¯
    full_msg_cmd = f'git log --format="%B" -n 1 {commit_hash}'
    full_message = run_git_command(full_msg_cmd)
    full_message_text = '\n'.join(full_message).strip()

    # è·å–ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
    files_cmd = f'git diff-tree --no-commit-id --name-only -r {commit_hash}'
    changed_files = run_git_command(files_cmd)
    files_list = ', '.join([f for f in changed_files if f.strip()])

    # è·å–æ–‡ä»¶å˜æ›´ç»Ÿè®¡
    stats_cmd = f'git diff-tree --no-commit-id --numstat {commit_hash}'
    stats = run_git_command(stats_cmd)

    file_stats = []
    for stat in stats:
        if stat.strip():
            parts = stat.split('\t')
            if len(parts) >= 3:
                added = parts[0] if parts[0] != '-' else '0'
                deleted = parts[1] if parts[1] != '-' else '0'
                filename = parts[2]
                file_stats.append(f"{filename}(+{added}/-{deleted})")

    detailed_files = ', '.join(file_stats)

    return {
        'full_message': clean_text_for_excel(full_message_text),
        'changed_files': clean_text_for_excel(files_list),
        'detailed_files': clean_text_for_excel(detailed_files)
    }

def parse_commit_info(commit_line):
    """è§£ææäº¤ä¿¡æ¯è¡Œ"""
    parts = commit_line.split('|')
    if len(parts) >= 4:
        return {
            'commit_hash': parts[0],
            'author': parts[1],
            'date': parts[2],
            'subject': '|'.join(parts[3:])  # å¤„ç†æ ‡é¢˜ä¸­å¯èƒ½åŒ…å«|çš„æƒ…å†µ
        }
    return None

def categorize_commit(commit_info, changed_files):
    """æ ¹æ®æäº¤ä¿¡æ¯å’Œä¿®æ”¹çš„æ–‡ä»¶å¯¹æäº¤è¿›è¡Œåˆ†ç±»"""
    categories = []
    subject = commit_info['subject'].lower()
    files = changed_files.lower()

    # RISC-Vç›¸å…³
    if any(keyword in subject for keyword in ['riscv', 'risc-v']) or 'arch/riscv' in files:
        categories.append('RISC-V')

    # è°ƒåº¦ç›¸å…³ - æ›´ä¸¥æ ¼çš„æ¡ä»¶
    sched_keywords = [
        'scheduler', 'sched:', 'sched_', 'cfs:', 'cfs_', 'rt:', 'rt_',
        'fair scheduler', 'rt scheduler', 'deadline scheduler',
        'load balancing', 'load balance', 'cpu scheduler',
        'task scheduler', 'process scheduler', 'thread scheduler',
        'runqueue', 'rq_', 'pick_next_task', 'enqueue_task', 'dequeue_task',
        'sched_domain', 'sched_group', 'sched_entity', 'sched_class',
        'wake_up_new_task', 'try_to_wake_up', 'schedule()', 'preempt'
    ]

    # æ–‡ä»¶è·¯å¾„æ£€æŸ¥ - åªæœ‰çœŸæ­£çš„è°ƒåº¦ç›¸å…³è·¯å¾„
    sched_paths = [
        'kernel/sched/',
        'include/linux/sched/',
        'include/uapi/linux/sched.h'
    ]

    # æ’é™¤æ˜æ˜¾ä¸æ˜¯è°ƒåº¦çš„å…³é”®è¯ï¼ˆå³ä½¿åŒ…å«schedç­‰è¯ï¼‰
    exclude_keywords = [
        # 'usched',  # ç”¨æˆ·æ€è°ƒåº¦
        # 'fsched',  # æ–‡ä»¶ç³»ç»Ÿè°ƒåº¦
        # 'iosched', # IOè°ƒåº¦
        # 'elevator', # ç”µæ¢¯è°ƒåº¦ç®—æ³•
        # 'deadline', # é™¤éæ˜ç¡®æ˜¯deadline scheduler
        # 'nohz',    # é™¤éæ˜¯è°ƒåº¦ç›¸å…³çš„nohz
        # 'rcu',     # RCUè°ƒåº¦
        # 'irq',     # ä¸­æ–­è°ƒåº¦
        # 'workqueue', # å·¥ä½œé˜Ÿåˆ—è°ƒåº¦
        # 'timer',   # å®šæ—¶å™¨è°ƒåº¦
        # 'hrtimer', # é«˜ç²¾åº¦å®šæ—¶å™¨
        # 'tick',    # æ—¶é’Ÿæ»´ç­”
        # 'clocksource', # æ—¶é’Ÿæº
        # 'cpufreq', # CPUé¢‘ç‡è°ƒåº¦
        # 'cpuidle', # CPUç©ºé—²
        # 'thermal', # çƒ­è°ƒåº¦
        # 'power',   # ç”µæºç®¡ç†è°ƒåº¦
        # 'suspend', # æŒ‚èµ·è°ƒåº¦
        # 'hibernate' # ä¼‘çœ è°ƒåº¦
    ]

    # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„è°ƒåº¦ç›¸å…³
    is_scheduler_related = False

    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶è·¯å¾„
    if any(path in files for path in sched_paths):
        is_scheduler_related = True

    # ç„¶åæ£€æŸ¥å…³é”®è¯ï¼Œä½†è¦æ’é™¤è¯¯æŠ¥
    elif any(keyword in subject for keyword in sched_keywords):
        # ç¡®ä¿ä¸åŒ…å«æ’é™¤çš„å…³é”®è¯
        if not any(exclude in subject for exclude in exclude_keywords):
            is_scheduler_related = True

    # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœåŒ…å«rtä½†æ˜ç¡®æ˜¯å…¶ä»–å­ç³»ç»Ÿçš„ï¼Œåˆ™æ’é™¤
    if 'rt' in subject:
        if any(exclude in subject for exclude in ['rtc', 'uart', 'spi', 'i2c', 'usb', 'pci', 'dma', 'gpio']):
            is_scheduler_related = False

    if is_scheduler_related:
        categories.append('è°ƒåº¦')

    # å†…å­˜ç®¡ç†
    if any(keyword in subject for keyword in ['mm', 'memory', 'page', 'slab', 'kmem']) or any(path in files for path in ['mm/', 'include/linux/mm']):
        categories.append('å†…å­˜ç®¡ç†')

    # æ–‡ä»¶ç³»ç»Ÿ
    if any(keyword in subject for keyword in ['fs', 'filesystem', 'ext4', 'btrfs', 'xfs']) or 'fs/' in files:
        categories.append('æ–‡ä»¶ç³»ç»Ÿ')

    # ç½‘ç»œ
    if any(keyword in subject for keyword in ['net', 'network', 'tcp', 'udp', 'socket']) or 'net/' in files:
        categories.append('ç½‘ç»œ')

    # é©±åŠ¨
    if 'drivers/' in files or any(keyword in subject for keyword in ['driver', 'device']):
        categories.append('é©±åŠ¨')

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•åˆ†ç±»ï¼Œå½’ä¸ºå…¶ä»–
    if not categories:
        categories.append('å…¶ä»–')

    return categories

def determine_patch_type(subject, full_message):
    """ç¡®å®šè¡¥ä¸ç±»å‹"""
    subject_lower = subject.lower()
    message_lower = full_message.lower()

    if any(keyword in subject_lower for keyword in ['fix', 'bug', 'error', 'issue']):
        return 'Bugä¿®å¤'
    elif any(keyword in subject_lower for keyword in ['add', 'new', 'implement', 'introduce']):
        return 'æ–°åŠŸèƒ½'
    elif any(keyword in subject_lower for keyword in ['improve', 'optimize', 'enhance', 'refactor']):
        return 'æ€§èƒ½ä¼˜åŒ–'
    elif any(keyword in subject_lower for keyword in ['update', 'change', 'modify']):
        return 'åŠŸèƒ½æ›´æ–°'
    else:
        return 'å…¶ä»–'

def format_worksheet(worksheet, df):
    """æ ¼å¼åŒ–å·¥ä½œè¡¨ï¼šè®¾ç½®å¯¹é½æ–¹å¼ã€è‡ªåŠ¨æ¢è¡Œå’Œè‡ªåŠ¨è°ƒæ•´åˆ—å®½"""
    # è®¾ç½®å¯¹é½æ–¹å¼ï¼šå·¦å¯¹é½ã€å‚ç›´å±…ä¸­ã€è‡ªåŠ¨æ¢è¡Œ
    alignment = Alignment(
        horizontal='left',
        vertical='center',
        wrap_text=True
    )

    # è®¾ç½®æ ‡é¢˜è¡Œå­—ä½“ä¸ºç²—ä½“
    header_font = Font(bold=True)

    # åº”ç”¨æ ¼å¼åˆ°æ‰€æœ‰å•å…ƒæ ¼
    for row in worksheet.iter_rows():
        for cell in row:
            cell.alignment = alignment
            # æ ‡é¢˜è¡Œè®¾ç½®ç²—ä½“
            if cell.row == 1:
                cell.font = header_font

    # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
    for column in worksheet.columns:
        max_length = 0
        column_letter = column[0].column_letter

        for cell in column:
            try:
                # è®¡ç®—å•å…ƒæ ¼å†…å®¹çš„æœ€å¤§é•¿åº¦
                if cell.value:
                    # å¤„ç†æ¢è¡Œç¬¦ï¼Œå–æœ€é•¿çš„ä¸€è¡Œ
                    lines = str(cell.value).split('\n')
                    cell_length = max(len(line) for line in lines) if lines else 0
                    max_length = max(max_length, cell_length)
            except:
                pass

        # è®¾ç½®åˆ—å®½ï¼Œé™åˆ¶æœ€å¤§å®½åº¦é¿å…è¿‡å®½
        adjusted_width = min(max_length + 2, 100)  # æœ€å¤§100å­—ç¬¦å®½åº¦
        if adjusted_width < 10:  # æœ€å°10å­—ç¬¦å®½åº¦
            adjusted_width = 10

        worksheet.column_dimensions[column_letter].width = adjusted_width

    # è®¾ç½®è¡Œé«˜è‡ªåŠ¨è°ƒæ•´ï¼ˆå¯¹äºæœ‰æ¢è¡Œçš„å†…å®¹ï¼‰
    for row in worksheet.iter_rows():
        max_lines = 1
        for cell in row:
            if cell.value:
                lines = str(cell.value).count('\n') + 1
                max_lines = max(max_lines, lines)

        # æ ¹æ®è¡Œæ•°è°ƒæ•´è¡Œé«˜ï¼ˆæ¯è¡Œå¤§çº¦15ä¸ªå•ä½ï¼‰
        if max_lines > 1:
            worksheet.row_dimensions[row[0].row].height = max_lines * 15

def create_formatted_excel(filename, dataframes_dict):
    """åˆ›å»ºæ ¼å¼åŒ–çš„Excelæ–‡ä»¶"""
    wb = Workbook()

    # åˆ é™¤é»˜è®¤çš„å·¥ä½œè¡¨
    if 'Sheet' in wb.sheetnames:
        wb.remove(wb['Sheet'])

    for sheet_name, df in dataframes_dict.items():
        # åˆ›å»ºå·¥ä½œè¡¨
        ws = wb.create_sheet(title=sheet_name)

        # å†™å…¥æ•°æ®
        for r in dataframe_to_rows(df, index=False, header=True):
            ws.append(r)

        # åº”ç”¨æ ¼å¼
        format_worksheet(ws, df)

    # ä¿å­˜æ–‡ä»¶
    wb.save(filename)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='åˆ†æä¸¤ä¸ªGitåˆ†æ”¯ä¹‹é—´çš„ç‹¬æœ‰è¡¥ä¸å·®å¼‚',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s v6.15.8 openkylin-6.6-next
  %(prog)s --source v6.15.8 --target openkylin-6.6-next
  %(prog)s v6.15.8 openkylin-6.6-next --output my_analysis.xlsx
  %(prog)s v6.15.8 openkylin-6.6-next --no-merge-base
        """
    )

    # ä½ç½®å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('source', nargs='?', help='æºåˆ†æ”¯ï¼ˆè¦åˆ†æå…¶ç‹¬æœ‰è¡¥ä¸çš„åˆ†æ”¯ï¼‰')
    parser.add_argument('target', nargs='?', help='ç›®æ ‡åˆ†æ”¯ï¼ˆç”¨äºæ¯”è¾ƒçš„åŸºå‡†åˆ†æ”¯ï¼‰')

    # å‘½åå‚æ•°
    parser.add_argument('-s', '--source', dest='source_named', help='æºåˆ†æ”¯ï¼ˆè¦åˆ†æå…¶ç‹¬æœ‰è¡¥ä¸çš„åˆ†æ”¯ï¼‰')
    parser.add_argument('-t', '--target', dest='target_named', help='ç›®æ ‡åˆ†æ”¯ï¼ˆç”¨äºæ¯”è¾ƒçš„åŸºå‡†åˆ†æ”¯ï¼‰')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤: <æºåˆ†æ”¯>-to-<ç›®æ ‡åˆ†æ”¯>-diff.xlsxï¼‰')
    parser.add_argument('--no-merge-base', action='store_true', help='ä¸ä½¿ç”¨å…¬å…±ç¥–å…ˆä¼˜åŒ–ï¼Œåˆ†æå…¨éƒ¨å·®å¼‚')
    parser.add_argument('-j', '--jobs', type=int, default=64, help='å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 64ï¼‰')
    parser.add_argument('--list-branches', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†æ”¯')

    args = parser.parse_args()

    # å¤„ç†åˆ†æ”¯å‚æ•°çš„ä¼˜å…ˆçº§ï¼šå‘½åå‚æ•° > ä½ç½®å‚æ•°
    source_branch = args.source_named or args.source
    target_branch = args.target_named or args.target

    return source_branch, target_branch, args

def list_available_branches():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†æ”¯"""
    print("ğŸ“‹ å¯ç”¨çš„åˆ†æ”¯åˆ—è¡¨:")

    # æœ¬åœ°åˆ†æ”¯
    local_cmd = 'git branch --format="%(refname:short)"'
    local_branches = run_git_command(local_cmd)
    if local_branches:
        print("\nğŸ  æœ¬åœ°åˆ†æ”¯:")
        for branch in local_branches:
            if branch.strip():
                print(f"  {branch.strip()}")

    # è¿œç¨‹åˆ†æ”¯
    remote_cmd = 'git branch -r --format="%(refname:short)"'
    remote_branches = run_git_command(remote_cmd)
    if remote_branches:
        print("\nğŸŒ è¿œç¨‹åˆ†æ”¯:")
        for branch in remote_branches:
            if branch.strip() and not branch.strip().endswith('/HEAD'):
                print(f"  {branch.strip()}")

    # æ ‡ç­¾
    tags_cmd = 'git tag --sort=-version:refname'
    tags = run_git_command(tags_cmd)
    if tags:
        print("\nğŸ·ï¸  æœ€è¿‘çš„æ ‡ç­¾ (å‰10ä¸ª):")
        for i, tag in enumerate(tags[:10]):
            if tag.strip():
                print(f"  {tag.strip()}")

def get_commit_details_batch(commit_hashes, max_workers=None):
    """æ‰¹é‡å¹¶è¡Œè·å–å¤šä¸ªæäº¤çš„è¯¦ç»†ä¿¡æ¯"""
    if max_workers is None:
        max_workers = min(len(commit_hashes), multiprocessing.cpu_count())

    results = {}

    def get_details_for_commit(commit_hash):
        return commit_hash, get_commit_details(commit_hash)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_commit = {executor.submit(get_details_for_commit, commit['commit_hash']): commit
                           for commit in commit_hashes}

        for future in as_completed(future_to_commit):
            try:
                commit_hash, details = future.result()
                results[commit_hash] = details
            except Exception as e:
                commit = future_to_commit[future]
                print(f"è·å–æäº¤ {commit['commit_hash']} è¯¦æƒ…æ—¶å‡ºé”™: {e}")
                # æä¾›é»˜è®¤å€¼
                results[commit['commit_hash']] = {
                    'full_message': '',
                    'changed_files': '',
                    'detailed_files': ''
                }

    return results

def analyze_commits_parallel(unique_commits, max_workers=None):
    """å¹¶è¡Œåˆ†ææäº¤ï¼ˆåˆ†ç±»å’Œç±»å‹ç¡®å®šï¼‰"""
    if max_workers is None:
        max_workers = min(len(unique_commits), multiprocessing.cpu_count())

    print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œåˆ†ææäº¤...")

    # 1. å¹¶è¡Œè·å–æ‰€æœ‰æäº¤çš„è¯¦ç»†ä¿¡æ¯
    print("ğŸ“¥ å¹¶è¡Œè·å–æäº¤è¯¦æƒ…...")
    details_dict = get_commit_details_batch(unique_commits, max_workers)

    # 2. å¹¶è¡Œè¿›è¡Œåˆ†ç±»å’Œç±»å‹åˆ†æ
    analysis_data = []
    analysis_lock = threading.Lock()

    def analyze_single_commit(commit):
        commit_hash = commit['commit_hash']
        details = details_dict.get(commit_hash, {
            'full_message': '',
            'changed_files': '',
            'detailed_files': ''
        })

        # åˆ†ç±»
        categories = categorize_commit(commit, details['changed_files'])

        # ç¡®å®šè¡¥ä¸ç±»å‹
        patch_type = determine_patch_type(commit['subject'], details['full_message'])

        # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»ºè®°å½•
        local_data = []
        for category in categories:
            local_data.append({
                'æäº¤å“ˆå¸Œ': commit['commit_hash'],
                'ä½œè€…': commit['author'],
                'æ—¥æœŸ': commit['date'],
                'æäº¤æ ‡é¢˜': commit['subject'],
                'å®Œæ•´æäº¤ä¿¡æ¯': details['full_message'],
                'ä¿®æ”¹æ–‡ä»¶': details['changed_files'],
                'æ–‡ä»¶å˜æ›´è¯¦æƒ…': details['detailed_files'],
                'åˆ†ç±»': category,
                'ç±»å‹': patch_type
            })

        # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ç»“æœä¸­
        with analysis_lock:
            analysis_data.extend(local_data)

    print("ğŸ” å¹¶è¡Œè¿›è¡Œåˆ†ç±»å’Œç±»å‹åˆ†æ...")
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰åˆ†æä»»åŠ¡
        futures = [executor.submit(analyze_single_commit, commit) for commit in unique_commits]

        # ç­‰å¾…å®Œæˆå¹¶æ˜¾ç¤ºè¿›åº¦
        completed = 0
        for future in as_completed(futures):
            try:
                future.result()
                completed += 1
                if completed % 20 == 0 or completed == len(futures):
                    progress = int(completed / len(futures) * 100)
                    print(f"åˆ†æè¿›åº¦: {progress}% ({completed}/{len(futures)})")
            except Exception as e:
                print(f"åˆ†ææäº¤æ—¶å‡ºé”™: {e}")
                completed += 1

    return analysis_data

def check_unique_commits_parallel(parsed_commits, target_patch_index, max_workers=None):
    """å¹¶è¡Œæ£€æŸ¥æäº¤çš„ç‹¬æœ‰æ€§"""
    if max_workers is None:
        max_workers = min(len(parsed_commits), multiprocessing.cpu_count())

    print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œæ£€æŸ¥ç‹¬æœ‰æ€§...")

    unique_commits = []
    equivalent_count = 0
    results_lock = threading.Lock()

    def check_commit_uniqueness(commit):
        nonlocal equivalent_count
        is_unique = is_patch_unique_fast(commit['commit_hash'], target_patch_index)

        with results_lock:
            if is_unique:
                unique_commits.append(commit)
            else:
                equivalent_count += 1

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æ£€æŸ¥ä»»åŠ¡
        futures = [executor.submit(check_commit_uniqueness, commit) for commit in parsed_commits]

        # ç­‰å¾…å®Œæˆå¹¶æ˜¾ç¤ºè¿›åº¦
        completed = 0
        for future in as_completed(futures):
            try:
                future.result()
                completed += 1
                if completed % 50 == 0 or completed == len(futures):
                    progress = int(completed / len(futures) * 100)
                    print(f"ç‹¬æœ‰æ€§æ£€æŸ¥è¿›åº¦: {progress}% ({completed}/{len(futures)})")
            except Exception as e:
                print(f"æ£€æŸ¥æäº¤ç‹¬æœ‰æ€§æ—¶å‡ºé”™: {e}")
                completed += 1

    return unique_commits, equivalent_count

# ä¿®æ”¹ä¸»å‡½æ•°ä¸­çš„ç›¸å…³éƒ¨åˆ†
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    source_branch, target_branch, args = parse_arguments()

    # å¦‚æœè¯·æ±‚åˆ—å‡ºåˆ†æ”¯
    if args.list_branches:
        list_available_branches()
        return

    # éªŒè¯å¿…éœ€çš„å‚æ•°
    if not source_branch or not target_branch:
        print("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®šæºåˆ†æ”¯å’Œç›®æ ‡åˆ†æ”¯")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python generate_patch_analysis.py <æºåˆ†æ”¯> <ç›®æ ‡åˆ†æ”¯>")
        print("  python generate_patch_analysis.py --source <æºåˆ†æ”¯> --target <ç›®æ ‡åˆ†æ”¯>")
        print("\næŸ¥çœ‹å¸®åŠ©: python generate_patch_analysis.py --help")
        print("åˆ—å‡ºåˆ†æ”¯: python generate_patch_analysis.py --list-branches")
        sys.exit(1)

    # éªŒè¯åˆ†æ”¯æ˜¯å¦å­˜åœ¨
    if not validate_branch_exists(source_branch):
        print(f"âŒ é”™è¯¯: æºåˆ†æ”¯ '{source_branch}' ä¸å­˜åœ¨")
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --list-branches æŸ¥çœ‹å¯ç”¨åˆ†æ”¯")
        sys.exit(1)

    if not validate_branch_exists(target_branch):
        print(f"âŒ é”™è¯¯: ç›®æ ‡åˆ†æ”¯ '{target_branch}' ä¸å­˜åœ¨")
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --list-branches æŸ¥çœ‹å¯ç”¨åˆ†æ”¯")
        sys.exit(1)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶åï¼Œåˆ™åŠ¨æ€ç”Ÿæˆ
    if not args.output:
        # æ¸…ç†åˆ†æ”¯åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…æ–‡ä»¶åé—®é¢˜
        safe_source = re.sub(r'[^\w\-_.]', '_', source_branch)
        safe_target = re.sub(r'[^\w\-_.]', '_', target_branch)
        args.output = f"{safe_source}-to-{safe_target}-diff.xlsx"

    print(f"ğŸ” åˆ†æåˆ†æ”¯å·®å¼‚: {target_branch}..{source_branch}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"ğŸ§µ å¹¶è¡Œçº¿ç¨‹: {args.jobs}")

    # 1. æ‰¾åˆ°å…¬å…±ç¥–å…ˆï¼ˆé™¤éç¦ç”¨ï¼‰
    merge_base = None
    if not args.no_merge_base:
        merge_base = find_merge_base(target_branch, source_branch)

    # 2. è·å–æºåˆ†æ”¯ç›¸å¯¹äºå…¬å…±ç¥–å…ˆçš„æäº¤å·®å¼‚
    if merge_base and not args.no_merge_base:
        commits_cmd = f'git log --pretty=format:"%h|%an|%ad|%s" --date=short --no-merges {merge_base}..{source_branch}'
        print(f"åªåˆ†æå…¬å…±ç¥–å…ˆ {merge_base[:8]} ä¹‹åçš„æäº¤")
    else:
        commits_cmd = f'git log --pretty=format:"%h|%an|%ad|%s" --date=short --no-merges {target_branch}..{source_branch}'
        print("åˆ†æå…¨éƒ¨å·®å¼‚æäº¤")

    commits = run_git_command(commits_cmd)

    if not commits:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°æäº¤å·®å¼‚ï¼Œä¸¤ä¸ªåˆ†æ”¯å†…å®¹ç›¸åŒ")
        return

    # è§£ææäº¤ä¿¡æ¯
    parsed_commits = []
    for commit in commits:
        if commit.strip():
            commit_info = parse_commit_info(commit)
            if commit_info:
                parsed_commits.append(commit_info)

    total_commits = len(parsed_commits)
    print(f"ğŸ“Š æ‰¾åˆ° {total_commits} ä¸ªæäº¤éœ€è¦åˆ†æ")

    if total_commits == 0:
        print("âœ… æ²¡æœ‰éœ€è¦åˆ†æçš„æäº¤")
        return

    # 3. æ„å»ºç›®æ ‡åˆ†æ”¯çš„patch-idç´¢å¼•
    target_patch_index = build_target_branch_patch_index(target_branch, merge_base)

    # 4. å¹¶è¡Œè¿‡æ»¤ç‹¬æœ‰è¡¥ä¸ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„ç´¢å¼•ï¼‰
    print(f"\nğŸ” å¼€å§‹å¹¶è¡Œæ£€æŸ¥ {total_commits} ä¸ªæäº¤çš„ç‹¬æœ‰æ€§...")

    # ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬
    unique_commits, equivalent_count = check_unique_commits_parallel(
        parsed_commits, target_patch_index, args.jobs
    )

    print(f"\nğŸ“ˆ è¿‡æ»¤ç»“æœ:")
    print(f"  æ€»æäº¤æ•°: {total_commits}")
    print(f"  ç­‰ä»·æäº¤: {equivalent_count}")
    print(f"  ç‹¬æœ‰è¡¥ä¸: {len(unique_commits)}")

    if not unique_commits:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°ç‹¬æœ‰è¡¥ä¸ï¼Œæ‰€æœ‰æäº¤éƒ½æœ‰ç­‰ä»·ç‰ˆæœ¬")
        return

    # 5. å¹¶è¡Œåˆ†ç±»å’Œåˆ†æç‹¬æœ‰è¡¥ä¸
    print(f"\nğŸ“ å¼€å§‹å¹¶è¡Œåˆ†æ {len(unique_commits)} ä¸ªç‹¬æœ‰è¡¥ä¸...")

    # ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬
    analysis_data = analyze_commits_parallel(unique_commits, args.jobs)

    # åˆ›å»ºDataFrame
    print("ğŸ“Š åˆ›å»ºæ•°æ®è¡¨...")
    df = pd.DataFrame(analysis_data)

    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    category_stats = df['åˆ†ç±»'].value_counts()
    type_stats = df['ç±»å‹'].value_counts()

    # å‡†å¤‡è¦å†™å…¥Excelçš„æ•°æ®å­—å…¸
    excel_data = {
        'ç‹¬æœ‰è¡¥ä¸è¯¦æƒ…': df,
        'åˆ†ç±»ç»Ÿè®¡': pd.DataFrame({
            'åˆ†ç±»': category_stats.index,
            'æ•°é‡': category_stats.values
        }),
        'ç±»å‹ç»Ÿè®¡': pd.DataFrame({
            'ç±»å‹': type_stats.index,
            'æ•°é‡': type_stats.values
        })
    }

    # æ·»åŠ å„åˆ†ç±»çš„ä¸“é—¨åˆ†æ
    for category in category_stats.index:
        category_df = df[df['åˆ†ç±»'] == category]
        if not category_df.empty:
            sheet_name = f'{category}ç‹¬æœ‰è¡¥ä¸'
            # Excelå·¥ä½œè¡¨åç§°é•¿åº¦é™åˆ¶
            if len(sheet_name) > 31:
                sheet_name = sheet_name[:28] + '...'
            excel_data[sheet_name] = category_df

    # åˆ›å»ºæ ¼å¼åŒ–çš„Excelæ–‡ä»¶
    print(f"ğŸ“„ ç”Ÿæˆæ ¼å¼åŒ–çš„ {args.output} æ–‡ä»¶...")
    try:
        create_formatted_excel(args.output, excel_data)

        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š åˆ†æ”¯å¯¹æ¯”: {source_branch} vs {target_branch}")
        if merge_base:
            print(f"ğŸ”— å…¬å…±ç¥–å…ˆ: {merge_base}")
        print(f"ğŸ“ˆ æ€»æäº¤æ•°: {total_commits}")
        print(f"ğŸ”„ ç­‰ä»·æäº¤: {equivalent_count}")
        print(f"â­ ç‹¬æœ‰è¡¥ä¸: {len(unique_commits)}")
        print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {len(patch_id_cache)} ä¸ªpatch-id")
        print(f"ğŸ§µ ä½¿ç”¨çº¿ç¨‹: {args.jobs}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
        print("\nğŸ“Š ç‹¬æœ‰è¡¥ä¸åˆ†ç±»ç»Ÿè®¡:")
        for category, count in category_stats.items():
            print(f"  {category}: {count} ä¸ªè¡¥ä¸")
        print("\nğŸ“‹ ç‹¬æœ‰è¡¥ä¸ç±»å‹ç»Ÿè®¡:")
        for patch_type, count in type_stats.items():
            print(f"  {patch_type}: {count} ä¸ªè¡¥ä¸")

    except Exception as e:
        print(f"âŒ ç”ŸæˆExcelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        csv_file = args.output.replace('.xlsx', '.csv')
        print(f"å°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼: {csv_file}")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"âœ… å·²ä¿å­˜ä¸ºCSVæ–‡ä»¶: {csv_file}")

if __name__ == "__main__":
    main()